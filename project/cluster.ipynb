{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"%load_ext autoreload\n%autoreload 2\nimport numpy as np\nfrom ours_eval import csls_knn_10_score, evaluation, separate_eva\nfrom icp import ICPTrainer\nimport matplotlib.pyplot as plt\nimport utils\nfrom utils import sub_icp, find_clts, find_centers, csls_knn_10_score\nimport params\nimport sklearn.cluster\nfrom sklearn.decomposition import PCA"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"def normalize(x):\n    return x / np.linalg.norm(x, ord=2, axis=1, keepdims=True)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Loaded 200000 pre-trained word embeddings.\nLoaded 200000 pre-trained word embeddings.\ndata/en-es.5000-6500.txt\nFound 2975 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\ndata/es-en.5000-6500.txt\nFound 2416 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n"}],"source":"src_id2word, src_word2id, src_embeddings = utils.read_txt_embeddings('data/wiki.%s.vec' % params.src_lang, params.n_eval_ex, False) #n_eval_ex = 200000\ntgt_id2word, tgt_word2id, tgt_embeddings = utils.read_txt_embeddings('data/wiki.%s.vec' % params.tgt_lang, params.n_eval_ex, False)\nsrc_normed = normalize(src_embeddings)\ntgt_normed = normalize(tgt_embeddings)\ncross_dict_src2tgt = utils.load_dictionary('data/%s-%s.5000-6500.txt' % (params.src_lang, params.tgt_lang), src_word2id, tgt_word2id)\ncross_dict_tgt2src = utils.load_dictionary('data/%s-%s.5000-6500.txt' % (params.tgt_lang, params.src_lang), tgt_word2id, src_word2id)"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"T = np.load(\"%s/%s_%s_T.npy\" % (params.cp_dir, params.src_lang, params.tgt_lang))\nT2 = np.load(\"%s/%s_%s_T.npy\" % (params.cp_dir, params.tgt_lang, params.src_lang))\nTranslatedX = src_embeddings.dot(np.transpose(T))"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":"'\\npca_25 = PCA(n_components = 25)\\npca_25.fit(src_full_normed)\\nsrc_W = pca_25.transform(src_full_normed)\\npca_25.fit(tgt_full_normed)\\ntgt_W = pca_25.transform(tgt_full_normed)\\n'"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":"src_full = np.load(\"data/%s_%d.npy\" % (params.src_lang, params.n_init_ex)) # 10000, 10000 english\nsrc_trans = src_full.dot(np.transpose(T))\ntgt_full = np.load(\"data/%s_%d.npy\" % (params.tgt_lang, params.n_init_ex)) # 300, 10000 es\n\n# for i in range(5):\nsrc_trans_normed = normalize(src_trans)\ntgt_full_normed = normalize(tgt_full)\n'''\npca_25 = PCA(n_components = 25)\npca_25.fit(src_full_normed)\nsrc_W = pca_25.transform(src_full_normed)\npca_25.fit(tgt_full_normed)\ntgt_W = pca_25.transform(tgt_full_normed)\n'''"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"n_clusters = 2\n\nsrc_clt = sklearn.cluster.KMeans(n_clusters=n_clusters, n_init= 40, random_state=200)\n'''\ntgt_clt= sklearn.cluster.KMeans(n_clusters=n_clusters, n_init= 40, random_state=200)\nsrc_clt = sklearn.cluster.SpectralClustering(n_clusters = n_clusters, n_init= 40, random_state=200)\n'''\nsrc_y = src_clt.fit_predict(src_trans_normed)\ntgt_y = src_clt.predict(tgt_full_normed)"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[(0, 2940), (1, 2060)]\n[(0, 3494), (1, 1506)]\n"}],"source":"src_centers, tgt_centers, src_dic, tgt_dic = find_centers(src_full, tgt_full, src_y, tgt_y, n_clts = n_clusters)\nTranslated_centers = src_centers.dot(np.transpose(T))"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[[0.93892501 0.76449506]\n [0.77669488 0.95015175]]\n"}],"source":"trans_c = normalize(Translated_centers)\ntgt_c = normalize(tgt_centers)\n#trans_c = Translated_centers\n#tgt_c = tgt\n\ncos_dist = trans_c.dot(tgt_c.T)\nprint(cos_dist)"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"mk_dic = {0:3, 1:1, 2:0, 3:2, 4:4}"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"data/en-es.5000-6500.txt\nFound 2975 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\ndata/es-en.5000-6500.txt\nFound 2416 pairs of words in the dictionary (1500 unique). 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n"}],"source":"cross_dict_src2tgt = utils.load_dictionary('data/%s-%s.5000-6500.txt' % (params.src_lang, params.tgt_lang), src_word2id, tgt_word2id)\ncross_dict_tgt2src = utils.load_dictionary('data/%s-%s.5000-6500.txt' % (params.tgt_lang, params.src_lang), tgt_word2id, src_word2id)"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"1.002 0.824\n"}],"source":"src_classes = find_clts(data = src_embeddings, centers = src_centers, dico = cross_dict_src2tgt)\ntgt_classes = find_clts(data = tgt_embeddings, centers = tgt_centers, dico = cross_dict_tgt2src)\n\nsrc_classes_trans = find_clts(data = src_embeddings, centers = src_centers, dico = cross_dict_src2tgt, trans = True)\nsrc_correct = np.where(src_classes == src_classes_trans)\nsrc_acc = src_correct[0].shape[0]/1500\ntgt_classes_trans = find_clts(data = tgt_embeddings, centers = tgt_centers, dico = cross_dict_tgt2src, trans = True)\ntgt_correct = np.where(tgt_classes == tgt_classes_trans)\ntgt_acc = tgt_correct[0].shape[0]/1500\nprint(src_acc, tgt_acc)"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"cluster 0 src shape: (2940, 300) tgt shape: (3494, 300)\n0: Rec 6.799466 BB 551 Time: 15.941349\n"}],"source":"TX, TY = utils.multi_ICP(src_full, tgt_full, src_y, tgt_y, src_dic, tgt_dic, n_clusters, time_run_icp = 5)"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"953 source words -   - Precision at k = 1: 0.000000\n953 source words -   - Precision at k = 5: 0.209864\n953 source words -   - Precision at k = 10: 0.209864\n547 source words -   - Precision at k = 1: 24.497258\n547 source words -   - Precision at k = 5: 44.058501\n547 source words -   - Precision at k = 10: 55.027422\n2447 source words -   - Precision at k = 1: 10.391563\n2447 source words -   - Precision at k = 5: 18.810144\n2447 source words -   - Precision at k = 10: 23.463082\n936 source words -   - Precision at k = 1: 0.000000\n936 source words -   - Precision at k = 5: 0.106838\n936 source words -   - Precision at k = 10: 0.106838\n564 source words -   - Precision at k = 1: 35.638298\n564 source words -   - Precision at k = 5: 56.560284\n564 source words -   - Precision at k = 10: 61.879433\n2069 source words -   - Precision at k = 1: 13.917711\n2069 source words -   - Precision at k = 5: 22.153423\n2069 source words -   - Precision at k = 10: 24.230693\n"}],"source":"result_src = separate_eva(src_embeddings, tgt_embeddings, T, TX, src_classes, dico=cross_dict_src2tgt)\nresult_tgt = separate_eva(tgt_embeddings, src_embeddings, T2, TY, tgt_classes, dico=cross_dict_tgt2src)"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}